{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto\n",
    "## Clasificación de Sonidos Urbanos\n",
    "\n",
    "**Universidad del Valle de Guatemala**\\\n",
    "**Facultad de Ingeniería**\\\n",
    "**Departamento de Ciencias de la Computación**\\\n",
    "**Deep Learning**\n",
    "\n",
    "---\n",
    "### Integrantes:\n",
    "- Diego Leiva\n",
    "- Pablo Orellana\n",
    "- Maria Marta Ramirez\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Utils\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import platform\n",
    "\n",
    "# Advertencias\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Ignorar advertencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = 'data/UrbanSound8K/audio/'\n",
    "\n",
    "# Obtencion de metadatos\n",
    "metadata = pd.read_csv('data/UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "files = metadata['slice_file_name'].values\n",
    "labels = metadata['classID'].values\n",
    "folds = metadata['fold'].values\n",
    "\n",
    "# Creacion de directorios\n",
    "paths = [\n",
    "    os.path.join(AUDIO_PATH + f\"fold{fold}\", file) for fold, file in zip(folds, files)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuracion de PyTorch CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semilla para reproducibilidad\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configuración de determinismo\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device_info = \"\"\n",
    "\n",
    "# Configuración de dispositivo\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    device_info = f'{torch.cuda.get_device_name(0)}'\n",
    "else:\n",
    "    device_info = f\"{platform.processor()}\"\n",
    "\n",
    "print(f\"Device: {device_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset y Dataloader de Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset():\n",
    "    def __init__(self, paths, labels):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.audio_length = 160000  # 4 segundos de audio\n",
    "\n",
    "        # Definir transformaciones con parámetros ajustados\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=22050,\n",
    "            n_fft=2048,\n",
    "            win_length=1024,\n",
    "            hop_length=512,\n",
    "            n_mels=120,  # Reducido de 128\n",
    "            f_min=0,\n",
    "            f_max=11025  # sample_rate/2\n",
    "        )\n",
    "        \n",
    "        self.mfcc = torchaudio.transforms.MFCC(\n",
    "            sample_rate=22050,\n",
    "            n_mfcc=48\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.paths[idx]\n",
    "        waveform, _ = torchaudio.load(file, normalize=True)\n",
    "        mono = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        temp = torch.zeros([1, self.audio_length])\n",
    "        if mono.numel() < self.audio_length:\n",
    "            temp[:, :mono.numel()] = mono\n",
    "        else:\n",
    "            temp = mono[:, :self.audio_length]\n",
    "\n",
    "        # Asignar audio mono \n",
    "        audio_mono = temp\n",
    "\n",
    "        # Obtener espectrograma de Mel\n",
    "        mel_spectrogram = self.mel_spec(audio_mono)\n",
    "        mel_spectrogram_norm = (mel_spectrogram - mel_spectrogram.mean()) / mel_spectrogram.std()\n",
    "\n",
    "        # Obtener MFCC\n",
    "        mfcc = self.mfcc(audio_mono)\n",
    "        mfcc_norm = (mfcc - mfcc.mean()) / mfcc.std()\n",
    "\n",
    "        # Ajustar el tamaño de MFCC para que coincida con el de Mel\n",
    "        if mfcc_norm.size(2) != mel_spectrogram_norm.size(2):\n",
    "            mfcc_norm = F.interpolate(mfcc_norm, size=mel_spectrogram_norm.size(2), mode='linear')\n",
    "\n",
    "        # Crear el feature vector\n",
    "        feature_vector = torch.cat([mel_spectrogram_norm, mfcc_norm], axis=1)\n",
    "\n",
    "        feature_dict = {\n",
    "            'feature_vector': feature_vector[0].permute(1, 0).clone().detach(),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_data(data):\n",
    "    # Inicializar listas de features y labels\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterar sobre los datos\n",
    "    for element in data:\n",
    "        # Extraer feature y label\n",
    "        feature = element[\"feature_vector\"].to(device)\n",
    "        label = element[\"label\"].to(device)\n",
    "\n",
    "        # Agregar a las listas\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Realizar padding de los features y convertir labels a tensor\n",
    "    feature = nn.utils.rnn.pad_sequence(features, batch_first=True, padding_value=0.)\n",
    "    labels = torch.stack(labels).long()  # Asegurar que labels sea Long\n",
    "\n",
    "    return feature, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioLSTM(nn.Module):\n",
    "    def __init__(self, feature_size, out_features, hidden_layers, layers, dropout):\n",
    "        super().__init__()\n",
    "        self.n_hidden = hidden_layers\n",
    "        self.n_layers = layers\n",
    "        self.n_feature = feature_size\n",
    "\n",
    "        # Capa LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.n_feature,\n",
    "            hidden_size=self.n_hidden,\n",
    "            num_layers=self.n_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Capa de dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Activacion\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Capas lineales (fully connected)\n",
    "        self.fc1 = nn.Linear(int(hidden_layers), int(hidden_layers/2))\n",
    "        self.fc2 = nn.Linear(int(hidden_layers/2), out_features)\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, x, hidden):\n",
    "        # X shape -> (batch_size, sequence_length, n_features)\n",
    "        l_out, l_hidden = self.lstm(x, hidden)\n",
    "\n",
    "        # Out shape -> (batch_size, sequence_length, n_hidden*direcction)\n",
    "        out = self.dropout(l_out)\n",
    "\n",
    "        # out shape -> (batch_size, out_features)\n",
    "        out = self.fc1(out) # Capa 1\n",
    "        out = self.fc2(out[:, -1, :]) # Capa 2\n",
    "\n",
    "        # Retornar el output y el hidden state\n",
    "        return out, l_hidden\n",
    "    \n",
    "\n",
    "    # Inicializar hidden state\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Obtener pesos de la primera capa\n",
    "        weight = next(self.parameters()).data\n",
    "        # Inicializar hidden state con ceros\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        # Retornar hidden state\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(state, path):\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, epoch, optimizer, device):\n",
    "    # Inicializar lista de perdidas, accuracy, labels y predicciones\n",
    "    losses = []\n",
    "    labels = []\n",
    "    predictions = []\n",
    "\n",
    "    # Establecer modelo en modo de entrenamiento\n",
    "    model.train()\n",
    "\n",
    "    # Crear barra de progreso\n",
    "    loop = tqdm(data_loader)\n",
    "\n",
    "    # Iterar sobre los datos\n",
    "    for batch_idx, (data, target) in enumerate(loop):\n",
    "        # Enviar datos al dispositivo\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Limpiar gradientes\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Inicializar hidden state y output\n",
    "        output, _ = model(data, model.init_hidden(data.size(0)))\n",
    "        \n",
    "        # Calcular perdida\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward() # Backpropagation\n",
    "\n",
    "        # Optimizar\n",
    "        optimizer.step()\n",
    "\n",
    "        # Agregar perdida\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Obtener mayor valor de output\n",
    "        winners = output.argmax(dim=1)\n",
    "        \n",
    "        # Agregar labels y predicciones\n",
    "        labels += torch.flatten(target).cpu().tolist()\n",
    "        predictions += torch.flatten(winners).cpu().tolist()\n",
    "\n",
    "        # Calcular accuracy\n",
    "        batch_accuracy = (winners == target).sum().float() / float(target.size(0))\n",
    "\n",
    "        # Actualizar barra de progreso\n",
    "        loop.set_description(f\"TRAIN -> Epoch {epoch} | Batch: {batch_idx}/{len(data_loader)} | Loss: {loss.item():.4f} | Accuracy: {batch_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    # Obtener promedio de perdidas y accuracy\n",
    "    total_loss = np.mean(losses)\n",
    "    total_accuracy = np.mean(np.array(labels) == np.array(predictions))\n",
    "\n",
    "    # Retornar perdidas y accuracy\n",
    "    return total_loss, total_accuracy, labels, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de Validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(data_loader, model, device):\n",
    "    # Configurar modelo en modo de evaluacion\n",
    "    model.eval()\n",
    "    \n",
    "    # Inicializar lista de perdidas, accuracy, labels\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    # Sin actualizar gradientes\n",
    "    with torch.no_grad():\n",
    "        # Crear barra de progreso\n",
    "        loop = tqdm(data_loader)\n",
    "\n",
    "        # Iterar sobre los datos\n",
    "        for batch_idx, (data, target) in enumerate(loop):\n",
    "            # Enviar datos al dispositivo\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Inicializar hidden state y output\n",
    "            output, _ = model(data, model.init_hidden(data.size(0)))\n",
    "\n",
    "            # Obtener mayor valor de output\n",
    "            winners = output.argmax(dim=1)\n",
    "\n",
    "            # Agregar lables y predicciones\n",
    "            labels += torch.flatten(target).cpu()\n",
    "            predictions += torch.flatten(winners).cpu()\n",
    "\n",
    "            # Calcular accuracy\n",
    "            batch_accuracy = (winners == target).sum().float() / float(target.size(0))\n",
    "\n",
    "            loop.set_description(f\"VALIDATE -> Batch: {batch_idx}/{len(data_loader)} | Accuracy: {batch_accuracy:.4f}\")\n",
    "\n",
    "    # Calcular accuracy total de toda la época de validación\n",
    "    total_accuracy = np.mean(np.array(labels) == np.array(predictions))\n",
    "\n",
    "    # Retornar accuracy\n",
    "    return total_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuracion inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes\n",
    "EPOCHS = 20 # Numero de epocas\n",
    "OUT_FEATURE = 10 # Numero de clases\n",
    "PATIENCE = 5 # Paciencia para Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion de Dataloaders y Datasets para 10 Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave One Group Out Cross Validation\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Precisiones de cada fold\n",
    "fold_accuracies = []\n",
    "\n",
    "# Loop de entrenamiento por epocas y folds\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(logo.split(paths, labels, folds)):\n",
    "    if fold_idx > 0:\n",
    "        print(f\"\\n\")\n",
    "    \n",
    "    print(\"-\"*15, f\" FOLD {fold_idx+1} \", \"-\"*15)\n",
    "\n",
    "    # Dividir datos en train y validation sets\n",
    "    train_paths = [paths[i] for i in train_idx]\n",
    "    val_paths = [paths[i] for i in val_idx]\n",
    "    train_labels = [labels[i] for i in train_idx]\n",
    "    val_labels = [labels[i] for i in val_idx]\n",
    "\n",
    "    # Crear datasets\n",
    "    train_dataset = AudioDataset(train_paths, train_labels)\n",
    "    val_dataset = AudioDataset(val_paths, val_labels)\n",
    "\n",
    "    # Crear dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False, collate_fn=collate_data)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=collate_data)\n",
    "\n",
    "    # Entrenamiento del Modelo\n",
    "    model = AudioLSTM(feature_size=168, \n",
    "                      out_features=OUT_FEATURE, \n",
    "                      hidden_layers=256, \n",
    "                      layers=2, \n",
    "                      dropout=0.3).to(device)\n",
    "    \n",
    "    # Crear optimizador AdamW y scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=PATIENCE)\n",
    "\n",
    "    # Inicializar mejor accuracy y epoca\n",
    "    best_accuracy = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Inicializar listas para etiquetas y predicciones acumuladas de todo el fold\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # Loop de entrenamiento\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Entrenar modelo y obtener perdidas y accuracy\n",
    "        epoch_train_loss, epoch_train_acc, epoch_labels, epoch_predictions = train_model(train_loader, model, epoch, optimizer, device)\n",
    "        \n",
    "        # Acumular etiquetas y predicciones para el fold\n",
    "        all_labels.extend(epoch_labels)\n",
    "        all_predictions.extend(epoch_predictions)\n",
    "\n",
    "        # Validar modelo y obtener accuracy\n",
    "        epoch_val_acc = validate(val_loader, model, device)\n",
    "\n",
    "        # Actualizar el scheduler y guardar el mejor modelo\n",
    "        scheduler.step(epoch_train_acc)\n",
    "        \n",
    "        # Guardar el mejor modelo\n",
    "        if epoch_val_acc > best_accuracy:\n",
    "            best_accuracy = epoch_val_acc\n",
    "            save_model({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'models/lstm_fold{fold_idx + 1}.pth')\n",
    "\n",
    "    # Agregar accuracy del fold a la lista\n",
    "    fold_accuracies.append(best_accuracy)\n",
    "\n",
    "    # Generar reporte de clasificacion\n",
    "    report = classification_report(torch.tensor(all_labels).numpy(), torch.tensor(all_predictions).numpy())\n",
    "    print(f\"Fold {fold_idx + 1} - Classification Report\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar resultados finales\n",
    "print(\"-\"*15 ,f\" FINAL RESULTS \", \"-\"*15)\n",
    "for i, acc in enumerate(fold_accuracies):\n",
    "    print(f\"   - Fold {i+1} Accuracy: {acc:.4f}\")\n",
    "print(\"\\n\")\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f\"Average Accuracy: {average_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoundClassifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
